
# -*- coding: utf-8 -*-
"""AutoEn_TL_PD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cXm4AAQz2SrXV5H3KZnp83K9Oy7nK_7p
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd

# Load the dataset
file_path = 'parkinsons1.xlsx'
data = pd.read_excel(file_path)

# Display the first few rows of the dataset to understand its structure
data.head()
# Load dataset and split features/labels
X = data.drop('Class', axis=1)
y = data['Class']

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 1. Pretraining - Autoencoder (Self-supervised learning)
def build_autoencoder():
    input_layer = layers.Input(shape=(X_train_scaled.shape[1],))
    encoded = layers.Dense(128, activation='relu')(input_layer)
    encoded = layers.Dense(64, activation='relu')(encoded)

    # Bottleneck - Encoded features
    bottleneck = layers.Dense(32, activation='relu')(encoded)

    # Decoding part
    decoded = layers.Dense(64, activation='relu')(bottleneck)
    decoded = layers.Dense(128, activation='relu')(decoded)
    decoded = layers.Dense(X_train_scaled.shape[1], activation='sigmoid')(decoded)

    autoencoder = models.Model(inputs=input_layer, outputs=decoded)
    encoder = models.Model(inputs=input_layer, outputs=bottleneck)  # We use this for feature extraction
    return autoencoder, encoder


autoencoder, encoder = build_autoencoder()

# Compile and train the autoencoder
autoencoder.compile(optimizer='adamw', loss='mse')
autoencoder.fit(X_train_scaled, X_train_scaled, epochs=150, batch_size=16, validation_split=0.2, verbose=1)

# 2. Feature Extraction using the Encoder (from pretraining)
X_train_encoded = encoder.predict(X_train_scaled)
X_test_encoded = encoder.predict(X_test_scaled)

# 3. Transfer Learning: Fine-tuning for PD detection
def build_classification_model():
    model = models.Sequential([
        layers.Input(shape=(X_train_encoded.shape[1],)),
        layers.Dense(32, activation='swish'),
        layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
    ])
    return model


model = build_classification_model()
model.compile(optimizer='adamw', loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping to avoid overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Fine-tune the classification model
model.fit(X_train_encoded, y_train, epochs=150, batch_size=16, validation_split=0.2, callbacks=[early_stopping], verbose=1)

# Evaluate on the test set
test_loss, test_accuracy = model.evaluate(X_test_encoded, y_test)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Predict and evaluate
y_pred = (model.predict(X_test_encoded) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(f"Final Test Accuracy: {accuracy:.4f}")

############################ Multi Layer Perceptron ################
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset
file_path = 'parkinsons1.xlsx'
data = pd.read_excel(file_path)

# Split the data into features (X) and target (y)
X = data.drop(columns=['Class'])
y = data['Class']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize the MLPClassifier
mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)

# Train the model
mlp.fit(X_train_scaled, y_train)

# Make predictions
y_pred = mlp.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_rep)

################################ LSTM #############################
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
import tensorflow as tf
from tensorflow.keras import layers, models

# Load the dataset
file_path = 'parkinsons1.xlsx'
data = pd.read_excel(file_path)

# Split the data into features (X) and target (y)
X = data.drop(columns=['Class'])
y = data['Class']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape data to be compatible with LSTM (samples, time steps, features)
# We treat each row as a sequence of features (time steps = number of features per row)
X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)

# Build the LSTM model
model = models.Sequential()

# LSTM layers
model.add(layers.LSTM(128, input_shape=(X_train_reshaped.shape[1], 1), return_sequences=True))
model.add(layers.LSTM(64))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_reshaped, y_train, epochs=20, batch_size=32, validation_data=(X_test_reshaped, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test_reshaped, y_test)
y_pred = (model.predict(X_test_reshaped) > 0.5).astype("int32")

# Print accuracy and classification report
print(f"Test Accuracy: {test_acc:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))


######################## Auto encoder ################
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import tensorflow as tf
from tensorflow.keras import layers, models

# Load the dataset
file_path = 'parkinsons1.xlsx'
data = pd.read_excel(file_path)

# Split the data into features (X) and target (y)
X = data.drop(columns=['Class'])
y = data['Class']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build the autoencoder
input_dim = X_train_scaled.shape[1]

# Encoder
encoder = models.Sequential([
    layers.InputLayer(input_shape=(input_dim,)),
    layers.Dense(16, activation='relu'),   # Compressed layer
    layers.Dense(8, activation='relu')     # Smaller encoding layer
])

# Decoder
decoder = models.Sequential([
    layers.InputLayer(input_shape=(8,)),
    layers.Dense(16, activation='relu'),
    layers.Dense(input_dim, activation='sigmoid')  # Reconstruct input data
])

# Combine encoder and decoder to form the autoencoder
autoencoder = models.Sequential([encoder, decoder])

# Compile the autoencoder
autoencoder.compile(optimizer='adam', loss='mse')

# Train the autoencoder
autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# Use the encoder part to compress the features
X_train_encoded = encoder.predict(X_train_scaled)
X_test_encoded = encoder.predict(X_test_scaled)

# Build a classifier using the encoded features
classifier = models.Sequential([
    layers.InputLayer(input_shape=(8,)),
    layers.Dense(32, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # Binary classification
])

# Compile the classifier
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the classifier
classifier.fit(X_train_encoded, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# Evaluate the classifier
test_loss, test_acc = classifier.evaluate(X_test_encoded, y_test)
y_pred = (classifier.predict(X_test_encoded) > 0.5).astype("int32")

# Print accuracy and classification report
print(f"Test Accuracy: {test_acc:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

###################### TabNet #########################


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
from pytorch_tabnet.tab_model import TabNetClassifier

# Load the dataset
file_path = 'parkinsons1.xlsx'
data = pd.read_excel(file_path)

# Split the data into features (X) and target (y)
X = data.drop(columns=['Class'])
y = data['Class']

# Encode labels if necessary
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Optional scaling for TabNet
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert to numpy arrays
X_train_scaled = np.array(X_train_scaled)
X_test_scaled = np.array(X_test_scaled)

# Initialize and train TabNet classifier
tabnet_clf = TabNetClassifier()

tabnet_clf.fit(
    X_train_scaled, y_train,
    eval_set=[(X_test_scaled, y_test)],
    eval_metric=['accuracy'],
    max_epochs=100,
    patience=10,
    batch_size=32,
    virtual_batch_size=16
)

# Make predictions
y_pred_prob = tabnet_clf.predict_proba(X_test_scaled)[:, 1]
y_pred = (y_pred_prob > 0.5).astype(int)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))